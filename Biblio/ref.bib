
@article{gomez-luna_benchmarking_2021,
	title = {Benchmarking a {New} {Paradigm}: {An} {Experimental} {Analysis} of a {Real} {Processing}-in-{Memory} {Architecture}},
	shorttitle = {Benchmarking a {New} {Paradigm}},
	url = {https://arxiv.org/abs/2105.03814v7},
	doi = {10.48550/arXiv.2105.03814},
	abstract = {Many modern workloads, such as neural networks, databases, and graph processing, are fundamentally memory-bound. For such workloads, the data movement between main memory and CPU cores imposes a significant overhead in terms of both latency and energy. A major reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of main memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new 3D-stacked memory technologies that integrate memory with a logic layer where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. This paper provides the first comprehensive analysis of the first publicly-available real-world PIM architecture. We make two key contributions. First, we conduct an experimental characterization of the UPMEM-based PIM system using microbenchmarks to assess various architecture limits such as compute throughput and memory bandwidth, yielding new insights. Second, we present PrIM, a benchmark suite of 16 workloads from different application domains (e.g., linear algebra, databases, graph processing, neural networks, bioinformatics).},
	language = {en},
	urldate = {2023-03-07},
	author = {Gómez-Luna, Juan and Hajj, Izzat El and Fernandez, Ivan and Giannoula, Christina and Oliveira, Geraldo F. and Mutlu, Onur},
	month = may,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\M4UDML6R\\Gómez-Luna 等 - 2021 - Benchmarking a New Paradigm An Experimental Analy.pdf:application/pdf},
}

@book{lavenier_variant_2020,
	title = {Variant {Calling} {Parallelization} on {Processor}-in-{Memory} {Architecture}},
	abstract = {In this paper, we introduce a new combination of software and hardware PIM (Process-in-Memory) architecture to accelerate the variant calling genomic process. PIM translates into bringing data intensive calculations directly where the data is: within the DRAM, enhanced with thousands of processing units. The energy consumption, in large part due to data movement, is significantly lowered at a marginal additional hardware cost. Such design allows an unprecedented level of parallelism to process billions of short reads. Experiments on real PIM devices developed by the UPMEM company show significant speed-up compared to pure software implementation. The PIM solution also compared nicely to FPGA or GPU based acceleration bringing similar to twice the processing speed but most importantly being 5 to 8 times cheaper to deploy with up to 6 times less power consumption.},
	author = {Lavenier, Dominique and Cimadomo, Remy and Jodin, Romaric},
	month = nov,
	year = {2020},
	doi = {10.1101/2020.11.03.366237},
	file = {已提交版本:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\RA429338\\Lavenier 等 - 2020 - Variant Calling Parallelization on Processor-in-Me.pdf:application/pdf},
}

@article{tu_multcim_2023,
	title = {{MulTCIM}: {Digital} {Computing}-in-{Memory}-{Based} {Multimodal} {Transformer} {Accelerator} {With} {Attention}-{Token}-{Bit} {Hybrid} {Sparsity}},
	issn = {0018-9200, 1558-173X},
	shorttitle = {{MulTCIM}},
	url = {https://ieeexplore.ieee.org/document/10226612/},
	doi = {10.1109/JSSC.2023.3305663},
	abstract = {Multimodal Transformers are emerging artificial intelligence (AI) models that comprehend a mixture of signals from different modalities like vision, natural language, and speech. The attention mechanism and massive matrix multiplications (MMs) cause high latency and energy. Prior work has shown that a digital computing-in-memory (CIM) network can be an efficient architecture to process Transformers while maintaining high accuracy. To further improve energy efficiency, attentiontoken-bit hybrid sparsity in multimodal Transformers can be exploited. The hybrid sparsity significantly reduces computation, but the irregularity also harms CIM utilization. To fully utilize the attention-token-bit hybrid sparsity of multimodal Transformers, we design a digital CIM-based accelerator called MulTCIM with three corresponding features: The long reuse elimination dynamically reshapes the attention pattern to improve CIM utilization. The runtime token pruner (RTP) removes insignificant tokens, and the modal-adaptive CIM network (MACN) exploits symmetric modal overlapping to reduce CIM idleness. The effective bitwidth-balanced CIM (EBB-CIM) macro balances input bits across in-memory multiply-accumulations (MACs) to reduce computation time. The fabricated MulTCIM consumes only 2.24 µJ/Token for the ViLBERT-base model, achieving 2.50×–5.91× lower energy than previous Transformer accelerators and digital CIM accelerators.},
	language = {en},
	urldate = {2023-10-05},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Tu, Fengbin and Wu, Zihan and Wang, Yiqi and Wu, Weiwei and Liu, Leibo and Hu, Yang and Wei, Shaojun and Yin, Shouyi},
	year = {2023},
	pages = {1--12},
	file = {Tu 等 - 2023 - MulTCIM Digital Computing-in-Memory-Based Multimo.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\AGWWVYLD\\Tu 等 - 2023 - MulTCIM Digital Computing-in-Memory-Based Multimo.pdf:application/pdf},
}

@article{abualigah_arithmetic_2021,
	title = {The {Arithmetic} {Optimization} {Algorithm}},
	volume = {376},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782520307945},
	doi = {10.1016/j.cma.2020.113609},
	abstract = {This work proposes a new meta-heuristic method called Arithmetic Optimization Algorithm (AOA) that utilizes the distribution behavior of the main arithmetic operators in mathematics including (Multiplication (M), Division (D), Subtraction (S), and Addition (A)). AOA is mathematically modeled and implemented to perform the optimization processes in a wide range of search spaces. The performance of AOA is checked on twenty-nine benchmark functions and several real-world engineering design problems to showcase its applicability. The analysis of performance, convergence behaviors, and the computational complexity of the proposed AOA have been evaluated by different scenarios. Experimental results show that the AOA provides very promising results in solving challenging optimization problems compared with eleven other well-known optimization algorithms. Source codes of AOA are publicly available at and .},
	urldate = {2023-10-27},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Abualigah, Laith and Diabat, Ali and Mirjalili, Seyedali and Abd Elaziz, Mohamed and Gandomi, Amir H.},
	month = apr,
	year = {2021},
	keywords = {Optimization, Meta-heuristics, Algorithm, AOA, Arithmetic Optimization Algorithm, Artificial Intelligence, Benchmark, Computational Intelligence, Genetic Algorithm, Grey Wolf Optimizer, Heuristic, Particle Swarm Optimization, Whale Optimization Algorithm},
	pages = {113609},
	file = {全文:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\QR3TNKWH\\Abualigah 等 - 2021 - The Arithmetic Optimization Algorithm.pdf:application/pdf;全文:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\IEUAHSJD\\Abualigah 等 - 2021 - The Arithmetic Optimization Algorithm.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\B82C6SQW\\S0045782520307945.html:text/html},
}

@article{abualigah_reptile_2022,
	title = {Reptile {Search} {Algorithm} ({RSA}): {A} nature-inspired meta-heuristic optimizer},
	volume = {191},
	issn = {0957-4174},
	shorttitle = {Reptile {Search} {Algorithm} ({RSA})},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421014810},
	doi = {10.1016/j.eswa.2021.116158},
	abstract = {This paper proposes a novel nature-inspired meta-heuristic optimizer, called Reptile Search Algorithm (RSA), motivated by the hunting behaviour of Crocodiles. Two main steps of Crocodile behaviour are implemented, such as encircling, which is performed by high walking or belly walking, and hunting, which is performed by hunting coordination or hunting cooperation. The mentioned search methods of the proposed RSA are unique compared to other existing algorithms. The performance of the proposed RSA is evaluated using twenty-three classical test functions, thirty CEC2017 test functions, ten CEC2019 test functions, and seven real-world engineering problems. The obtained results of the proposed RSA are compared to various existing optimization algorithms in the literature. The results of the tested three benchmark functions revealed that the proposed RSA achieved better results than the other competitive optimization algorithms. The results of the Friedman ranking test proved that the RSA is a significantly superior method than other comparative methods. Finally, the results of the examined engineering problems showed that the RSA obtained better results compared to other various methods. Source codes of RSA are publicly available at https://www.mathworks.com/matlabcentral/fileexchange/101385-reptile-search-algorithm-rsa-a-nature-inspired-optimizer},
	urldate = {2023-10-27},
	journal = {Expert Systems with Applications},
	author = {Abualigah, Laith and Elaziz, Mohamed Abd and Sumari, Putra and Geem, Zong Woo and Gandomi, Amir H.},
	month = apr,
	year = {2022},
	keywords = {Meta-heuristics, Optimization algorithms, Optimization problems, Real-word problems, Reptile Search Algorithm (RSA)},
	pages = {116158},
}

@inproceedings{kennedy_particle_1995,
	title = {Particle swarm optimization},
	volume = {4},
	url = {https://ieeexplore.ieee.org/document/488968},
	doi = {10.1109/ICNN.1995.488968},
	abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.},
	urldate = {2023-10-27},
	booktitle = {Proceedings of {ICNN}'95 - {International} {Conference} on {Neural} {Networks}},
	author = {Kennedy, J. and Eberhart, R.},
	month = nov,
	year = {1995},
	pages = {1942--1948 vol.4},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\GD2BBM2K\\Kennedy 和 Eberhart - 1995 - Particle swarm optimization.pdf:application/pdf},
}

@inproceedings{ahn_pim-enabled_2015,
	address = {Portland Oregon},
	title = {{PIM}-enabled instructions: a low-overhead, locality-aware processing-in-memory architecture},
	isbn = {978-1-4503-3402-0},
	shorttitle = {{PIM}-enabled instructions},
	url = {https://dl.acm.org/doi/10.1145/2749469.2750385},
	doi = {10.1145/2749469.2750385},
	abstract = {Processing-in-memory (PIM) is rapidly rising as a viable solution for the memory wall crisis, rebounding from its unsuccessful attempts in 1990s due to practicality concerns, which are alleviated with recent advances in 3D stacking technologies. However, it is still challenging to integrate the PIM architectures with existing systems in a seamless manner due to two common characteristics: unconventional programming models for in-memory computation units and lack of ability to utilize large on-chip caches.},
	language = {en},
	urldate = {2023-10-27},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Ahn, Junwhan and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
	month = jun,
	year = {2015},
	pages = {336--348},
	file = {Ahn 等 - 2015 - PIM-enabled instructions a low-overhead, locality.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\JVEVLT4T\\Ahn 等 - 2015 - PIM-enabled instructions a low-overhead, locality.pdf:application/pdf},
}

@article{arabnejad_list_2014,
	title = {List {Scheduling} {Algorithm} for {Heterogeneous} {Systems} by an {Optimistic} {Cost} {Table}},
	volume = {25},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/6471969},
	doi = {10.1109/TPDS.2013.57},
	abstract = {Efficient application scheduling algorithms are important for obtaining high performance in heterogeneous computing systems. In this paper, we present a novel list-based scheduling algorithm called Predict Earliest Finish Time (PEFT) for heterogeneous computing systems. The algorithm has the same time complexity as the state-of-the-art algorithm for the same purpose, that is, O(v2.p) for v tasks and p processors, but offers significant makespan improvements by introducing a look-ahead feature without increasing the time complexity associated with computation of an optimistic cost table (OCT). The calculated value is an optimistic cost because processor availability is not considered in the computation. Our algorithm is only based on an OCT that is used to rank tasks and for processor selection. The analysis and experiments based on randomly generated graphs with various characteristics and graphs of real-world applications show that the PEFT algorithm outperforms the state-of-the-art list-based algorithms for heterogeneous systems in terms of schedule length ratio, efficiency, and frequency of best results.},
	number = {3},
	urldate = {2023-10-27},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Arabnejad, Hamid and Barbosa, Jorge G.},
	month = mar,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {682--694},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\7UHARHQ2\\6471969.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\7IZQ7N4J\\Arabnejad 和 Barbosa - 2014 - List Scheduling Algorithm for Heterogeneous System.pdf:application/pdf},
}

@article{zhou_list_2017,
	title = {A list scheduling algorithm for heterogeneous systems based on a critical node cost table and pessimistic cost table},
	volume = {29},
	copyright = {Copyright © 2016 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3944},
	doi = {10.1002/cpe.3944},
	abstract = {This paper presents a novel list-based scheduling algorithm called Improved Predict Earliest Finish Time for static task scheduling in a heterogeneous computing environment. The algorithm calculates the task priority with a pessimistic cost table, implements the feature prediction with a critical node cost table, and assigns the best processor for the node that has at least 1 immediate successor as the critical node, thereby effectively reducing the schedule makespan without increasing the algorithm time complexity. Experiments regarding aspects of randomly generated graphs and real-world application graphs are performed, and comparisons are made based on the scheduling length ratio, robustness, and frequency of the best result. The results demonstrate that the Improved Predict Earliest Finish Time algorithm outperforms the Predict Earliest Finish Time and Heterogeneous Earliest Finish Time algorithms in terms of the schedule length ratio, frequency of the best result, and robustness while maintaining the same time complexity.},
	language = {en},
	number = {5},
	urldate = {2023-10-27},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Zhou, Naqin and Qi, Deyu and Wang, Xinyang and Zheng, Zhishuo and Lin, Weiwei},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3944},
	keywords = {heterogeneous systems, DAG scheduling, list scheduling, static scheduling, task graphs},
	pages = {e3944},
	annote = {e3944 CPE-15-0408.R2},
	file = {Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\T6QDBGME\\cpe.html:text/html},
}

@inproceedings{bittencourt_dag_2010,
	title = {{DAG} {Scheduling} {Using} a {Lookahead} {Variant} of the {Heterogeneous} {Earliest} {Finish} {Time} {Algorithm}},
	url = {https://ieeexplore.ieee.org/document/5452513},
	doi = {10.1109/PDP.2010.56},
	abstract = {Among the numerous DAG scheduling heuristics suitable for heterogeneous systems, the Heterogeneous Earliest Finish Time (HEFT) heuristic is known to give good results in short time. In this paper, we propose an improvement of HEFT, where the locally optimal decisions made by the heuristic do not rely on estimates of a single task only, but also look ahead in the schedule and take into account information about the impact of this decision to the children of the task being allocated. Preliminary simulation results indicate that the lookahead variation of HEFT can effectively reduce the makespan of the schedule in most cases without making the algorithm's execution time prohibitively high.},
	urldate = {2023-10-27},
	booktitle = {2010 18th {Euromicro} {Conference} on {Parallel}, {Distributed} and {Network}-based {Processing}},
	author = {Bittencourt, Luiz F. and Sakellariou, Rizos and Madeira, Edmundo R. M.},
	month = feb,
	year = {2010},
	note = {ISSN: 2377-5750},
	pages = {27--34},
	file = {已提交版本:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\FQ8NCLZZ\\Bittencourt 等 - 2010 - DAG Scheduling Using a Lookahead Variant of the He.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\MFDWV43D\\5452513.html:text/html},
}

@article{topcuoglu_performance-effective_2002,
	title = {Performance-effective and low-complexity task scheduling for heterogeneous computing},
	volume = {13},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/993206},
	doi = {10.1109/71.993206},
	abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.},
	number = {3},
	urldate = {2023-10-27},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
	month = mar,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {260--274},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\Z5IKIW5H\\993206.html:text/html},
}

@article{ullman_np-complete_1975,
	title = {{NP}-complete scheduling problems},
	volume = {10},
	issn = {00220000},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022000075800080},
	doi = {10.1016/S0022-0000(75)80008-0},
	language = {en},
	number = {3},
	urldate = {2023-10-27},
	journal = {Journal of Computer and System Sciences},
	author = {Ullman, J.D.},
	month = jun,
	year = {1975},
	pages = {384--393},
	file = {Ullman - 1975 - NP-complete scheduling problems.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\F7KMQLPQ\\Ullman - 1975 - NP-complete scheduling problems.pdf:application/pdf},
}

@inproceedings{niu_184qpsw_2022,
	title = {{184QPS}/{W} {64Mb}/{mm23D} {Logic}-to-{DRAM} {Hybrid} {Bonding} with {Process}-{Near}-{Memory} {Engine} for {Recommendation} {System}},
	volume = {65},
	url = {https://ieeexplore.ieee.org/document/9731694},
	doi = {10.1109/ISSCC42614.2022.9731694},
	abstract = {The era of AI computing brings significant challenges to traditional computer systems. As shown in Fig. 29.1.1, while the AI model computation requirement increases 750x every two years, we only observe a very slow-paced improvement of memory system capability in terms of both capacity and bandwidth. There are many memory-bound applications, such as natural language processing, recommendation systems, graph analytics, graph neural networks, as well as multi-task online inference, that become dominating AI applications in modern cloud datacenters. Current primary memory technologies that power AI systems and applications include on-chip memory (SRAM), 2.5D integrated memory (HBM [1]), and off-chip memory (DDR, LPDDR, or GDDR SDRAM). Although on-chip memory enjoys low energy access compared to off-chip memory, limited on-chip memory capacity prevents the efficient adoption of large AI models due to intensive and costly off-chip memory access. In addition, the energy consumption of data movement of off-chip memory solutions (HBM and DRAM) is several orders of magnitude larger than that of on-chip memory, bringing the well-known “memory wall [2]“problem to AI systems. Process-near-memory (PNM) and computing-in-memory (CIM) have become promising candidates to tackle the “memory wall” problem in recent years.},
	urldate = {2023-10-27},
	booktitle = {2022 {IEEE} {International} {Solid}-{State} {Circuits} {Conference} ({ISSCC})},
	author = {Niu, Dimin and Li, Shuangchen and Wang, Yuhao and Han, Wei and Zhang, Zhe and Guan, Yijin and Guan, Tianchan and Sun, Fei and Xue, Fei and Duan, Lide and Fang, Yuanwei and Zheng, Hongzhong and Jiang, Xiping and Wang, Song and Zuo, Fengguo and Wang, Yubing and Yu, Bing and Ren, Qiwei and Xie, Yuan},
	month = feb,
	year = {2022},
	note = {ISSN: 2376-8606},
	pages = {1--3},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\EKTERL2Y\\9731694.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\V63DSU4A\\Niu 等 - 2022 - 184QPSW 64Mbmm23D Logic-to-DRAM Hybrid Bonding w.pdf:application/pdf},
}

@misc{mutlu_modern_2022,
	title = {A {Modern} {Primer} on {Processing} in {Memory}},
	url = {http://arxiv.org/abs/2012.03112},
	doi = {10.48550/arXiv.2012.03112},
	abstract = {Modern computing systems are overwhelmingly designed to move data to computation. This design choice goes directly against at least three key trends in computing that cause performance, scalability and energy bottlenecks: (1) data access is a key bottleneck as many important applications are increasingly data-intensive, and memory bandwidth and energy do not scale well, (2) energy consumption is a key limiter in almost all computing platforms, especially server and mobile systems, (3) data movement, especially off-chip to on-chip, is very expensive in terms of bandwidth, energy and latency, much more so than computation. These trends are especially severely-felt in the data-intensive server and energy-constrained mobile systems of today. At the same time, conventional memory technology is facing many technology scaling challenges in terms of reliability, energy, and performance. As a result, memory system architects are open to organizing memory in different ways and making it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic, the adoption of error correcting codes inside the latest DRAM chips, proliferation of different main memory standards and chips, specialized for different purposes (e.g., graphics, low-power, high bandwidth, low latency), and the necessity of designing new solutions to serious reliability and security issues, such as the RowHammer phenomenon, are an evidence of this trend. This chapter discusses recent research that aims to practically enable computation close to data, an approach we call processing-in-memory (PIM). PIM places computation mechanisms in or near where the data is stored (i.e., inside the memory chips, in the logic layer of 3D-stacked memory, or in the memory controllers), so that data movement between the computation units and memory is reduced or eliminated.},
	urldate = {2023-10-27},
	publisher = {arXiv},
	author = {Mutlu, Onur and Ghose, Saugata and Gómez-Luna, Juan and Ausavarungnirun, Rachata},
	month = aug,
	year = {2022},
	note = {arXiv:2012.03112 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1903.03988},
	file = {全文:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\ZTJGDIFI\\Mutlu 等 - 2022 - A Modern Primer on Processing in Memory.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\E4IT63EQ\\Mutlu 等 - 2022 - A Modern Primer on Processing in Memory.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\9U3TSF6Z\\2012.html:text/html},
}

@article{marco-sola_fast_2020,
	title = {Fast gap-affine pairwise alignment using the wavefront algorithm},
	copyright = {Attribution-NonCommercial 4.0},
	issn = {1367-4803},
	url = {https://upcommons.upc.edu/handle/2117/330104},
	doi = {10.1093/bioinformatics/btaa777},
	language = {eng},
	number = {btaa777},
	urldate = {2023-10-27},
	journal = {Bioinformatics},
	author = {Marco-Sola, Santiago and Moure López, Juan Carlos and Moreto Planas, Miquel and Espinosa Morales, Antonio},
	month = sep,
	year = {2020},
	note = {Accepted: 2020-10-09T11:22:56Z},
	keywords = {Algorismes, Algorithms, Àrees temàtiques de la UPC::Informàtica::Aplicacions de la informàtica::Bioinformàtica, Genòmica, Genomics},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\X68AUBFK\\Marco-Sola 等 - 2020 - Fast gap-affine pairwise alignment using the wavef.pdf:application/pdf},
}

@misc{gomez-luna_experimental_2023,
	title = {An {Experimental} {Evaluation} of {Machine} {Learning} {Training} on a {Real} {Processing}-in-{Memory} {System}},
	url = {http://arxiv.org/abs/2207.07886},
	abstract = {Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck. Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real memory-centric computing system with more than 2500 PIM cores shows that general-purpose PIM architectures can greatly accelerate memory-bound ML workloads, when the necessary operations and datatypes are natively supported by PIM hardware. For example, our PIM implementation of decision tree is \$27{\textbackslash}times\$ faster than a state-of-the-art CPU version on an 8-core Intel Xeon, and \$1.34{\textbackslash}times\$ faster than a state-of-the-art GPU version on an NVIDIA A100. Our K-Means clustering on PIM is \$2.8{\textbackslash}times\$ and \$3.2{\textbackslash}times\$ than state-of-the-art CPU and GPU versions, respectively. To our knowledge, our work is the first one to evaluate ML training on a real-world PIM architecture. We conclude with key observations, takeaways, and recommendations that can inspire users of ML workloads, programmers of PIM architectures, and hardware designers \& architects of future memory-centric computing systems.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Gómez-Luna, Juan and Guo, Yuxin and Brocard, Sylvan and Legriel, Julien and Cimadomo, Remy and Oliveira, Geraldo F. and Singh, Gagandeep and Mutlu, Onur},
	month = sep,
	year = {2023},
	note = {arXiv:2207.07886 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	annote = {Comment: Our open-source software is available at https://github.com/CMU-SAFARI/pim-ml},
	file = {arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\TEP32T69\\2207.html:text/html;Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\5IRDFIFR\\Gómez-Luna 等 - 2023 - An Experimental Evaluation of Machine Learning Tra.pdf:application/pdf},
}

@inproceedings{das_implementation_2022,
	title = {Implementation and {Evaluation} of {Deep} {Neural} {Networks} in {Commercially} {Available} {Processing} in {Memory} {Hardware}},
	url = {https://ieeexplore.ieee.org/abstract/document/9908126},
	doi = {10.1109/SOCC56010.2022.9908126},
	abstract = {Deep Neural Networks (DNNs) are often associated with a large number of data-parallel computations. Therefore, data-centric computing paradigms, such as Processing in Memory (PIM), are being widely explored for DNN acceleration applications. A recent PIM architecture, developed and commercialized by the UPMEM company, has demonstrated impressive performance boost over traditional CPU-based systems for a wide range of data-parallel applications. However, the application domain of DNN acceleration is yet to be explored on this PIM platform. In this work, we present successful implementations of DNNs on the UPMEM PIM system. We explore multiple operation mapping schemes with different optimization goals and accelerate two CNN algorithms using these schemes. Based on the data achieved from the physical implementation of the DNNs on the UPMEM system, we compare the performance of our DNN implementation with several other recently proposed PIM architecture.},
	urldate = {2023-10-25},
	booktitle = {2022 {IEEE} 35th {International} {System}-on-{Chip} {Conference} ({SOCC})},
	author = {Das, Prangon and Sutradhar, Purab Ranjan and Indovina, Mark and Dinakarrao, Sai Manoj Pudukotai and Ganguly, Amlan},
	month = sep,
	year = {2022},
	note = {ISSN: 2164-1706},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\UPLLBGHJ\\Das 等 - 2022 - Implementation and Evaluation of Deep Neural Netwo.pdf:application/pdf},
}

@misc{kang_pim-tree_2022,
	title = {{PIM}-tree: {A} {Skew}-resistant {Index} for {Processing}-in-{Memory}},
	shorttitle = {{PIM}-tree},
	url = {http://arxiv.org/abs/2211.10516},
	abstract = {The performance of today's in-memory indexes is bottlenecked by the memory latency/bandwidth wall. Processing-in-memory (PIM) is an emerging approach that potentially mitigates this bottleneck, by enabling low-latency memory access whose aggregate memory bandwidth scales with the number of PIM nodes. There is an inherent tension, however, between minimizing inter-node communication and achieving load balance in PIM systems, in the presence of workload skew. This paper presents PIM-tree, an ordered index for PIM systems that achieves both low communication and high load balance, regardless of the degree of skew in the data and the queries. Our skew-resistant index is based on a novel division of labor between the multi-core host CPU and the PIM nodes, which leverages the strengths of each. We introduce push-pull search, which dynamically decides whether to push queries to a PIM-tree node (CPU -{\textgreater} PIM-node) or pull the node's keys back to the CPU (PIM-node -{\textgreater} CPU) based on workload skew. Combined with other PIM-friendly optimizations (shadow subtrees and chunked skip lists), our PIM-tree provides high-throughput, (guaranteed) low communication, and (guaranteed) high load balance, for batches of point queries, updates, and range scans. We implement the PIM-tree structure, in addition to prior proposed PIM indexes, on the latest PIM system from UPMEM, with 32 CPU cores and 2048 PIM nodes. On workloads with 500 million keys and batches of one million queries, the throughput using PIM-trees is up to 69.7x and 59.1x higher than the two best prior methods. As far as we know these are the first implementations of an ordered index on a real PIM system.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Kang, Hongbo and Zhao, Yiwei and Blelloch, Guy E. and Dhulipala, Laxman and Gu, Yan and McGuffey, Charles and Gibbons, Phillip B.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.10516 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, 68P05, Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Performance, H.2.4},
	file = {arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\BLH7YHZB\\2211.html:text/html;Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\XGKDUE4W\\Kang 等 - 2022 - PIM-tree A Skew-resistant Index for Processing-in.pdf:application/pdf},
}

@inproceedings{devaux_true_2019,
	title = {The true {Processing} {In} {Memory} accelerator},
	url = {https://ieeexplore.ieee.org/document/8875680},
	doi = {10.1109/HOTCHIPS.2019.8875680},
	abstract = {This article consists of a collection of slides from the author's conference presentation.},
	urldate = {2023-10-25},
	booktitle = {2019 {IEEE} {Hot} {Chips} 31 {Symposium} ({HCS})},
	author = {Devaux, Fabrice},
	month = aug,
	year = {2019},
	note = {ISSN: 2573-2048},
	pages = {1--24},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\YJP2IKGT\\8875680.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\BUL5VKUY\\Devaux - 2019 - The true Processing In Memory accelerator.pdf:application/pdf},
}

@misc{abecassis_gapim_2023,
	title = {{GAPiM}: {Discovering} {Genetic} {Variations} on a {Real} {Processing}-in-{Memory} {System}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{GAPiM}},
	url = {https://www.biorxiv.org/content/10.1101/2023.07.26.550623v1},
	doi = {10.1101/2023.07.26.550623},
	abstract = {Variant calling is a fundamental stage in genome analysis that identifies mutations (variations) in a sequenced genome relative to a known reference genome. Pair-HMM is a key part of the variant calling algorithm and its most compute-intensive part. In recent years, Processing-in-Memory (PiM) solutions, which consist of placing compute capabilities near/inside memory, have been proposed to speed up the genome analysis pipeline. We implement the Pair-HMM algorithm on a commercial PiM platform developed by UPMEM. We modify the Pair-HMM algorithm to make it more suitable for PiM execution with acceptable loss of accuracy. We evaluate our implementation on single chromosomes and whole genome sequencing datasets, demonstrating up to 2x speedup compared to existing CPU accelerations and up to 3x speedup compared to FPGA accelerations.},
	language = {en},
	urldate = {2023-10-25},
	publisher = {bioRxiv},
	author = {Abecassis, Naomie and Gómez-Luna, Juan and Mutlu, Onur and Ginosar, Ran and Moisson-Franckhauser, Aphélie and Yavits, Leonid},
	month = jul,
	year = {2023},
	note = {Pages: 2023.07.26.550623
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\T5ZBLSGT\\Abecassis 等 - 2023 - GAPiM Discovering Genetic Variations on a Real Pr.pdf:application/pdf},
}

@misc{diab_framework_2023,
	title = {A {Framework} for {High}-throughput {Sequence} {Alignment} using {Real} {Processing}-in-{Memory} {Systems}},
	url = {http://arxiv.org/abs/2208.01243},
	doi = {10.48550/arXiv.2208.01243},
	abstract = {Sequence alignment is a memory bound computation whose performance in modern systems is limited by the memory bandwidth bottleneck. Processing-in-memory architectures alleviate this bottleneck by providing the memory with computing competencies. We propose Alignment-in-Memory (AIM), a framework for high-throughput sequence alignment using processing-in-memory, and evaluate it on UPMEM, the first publicly-available general-purpose programmable processing-in-memory system. Our evaluation shows that a real processing-in-memory system can substantially outperform server-grade multi-threaded CPU systems running at full-scale when performing sequence alignment for a variety of algorithms, read lengths, and edit distance thresholds. We hope that our findings inspire more work on creating and accelerating bioinformatics algorithms for such real processing-in-memory systems. Our code is available at https://github.com/safaad/aim.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Diab, Safaa and Nassereldine, Amir and Alser, Mohammed and Gómez-Luna, Juan and Mutlu, Onur and Hajj, Izzat El},
	month = mar,
	year = {2023},
	note = {arXiv:2208.01243 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\ZC8Q6NTH\\Diab 等 - 2023 - A Framework for High-throughput Sequence Alignment.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\D5C7LB5H\\2208.html:text/html},
}

@misc{diab_high-throughput_2022,
	title = {High-throughput {Pairwise} {Alignment} with the {Wavefront} {Algorithm} using {Processing}-in-{Memory}},
	url = {http://arxiv.org/abs/2204.02085},
	doi = {10.48550/arXiv.2204.02085},
	abstract = {We show that the wavefront algorithm can achieve higher pairwise read alignment throughput on a UPMEM PIM system than on a server-grade multi-threaded CPU system.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Diab, Safaa and Nassereldine, Amir and Alser, Mohammed and Luna, Juan Gómez and Mutlu, Onur and Hajj, Izzat El},
	month = apr,
	year = {2022},
	note = {arXiv:2204.02085 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\8QYZSNGC\\Diab 等 - 2022 - High-throughput Pairwise Alignment with the Wavefr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\TH94SZFC\\2204.html:text/html},
}

@misc{giannoula_sparsep_2022,
	title = {{SparseP}: {Towards} {Efficient} {Sparse} {Matrix} {Vector} {Multiplication} on {Real} {Processing}-{In}-{Memory} {Systems}},
	shorttitle = {{SparseP}},
	url = {http://arxiv.org/abs/2201.05072},
	doi = {10.48550/arXiv.2201.05072},
	abstract = {Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures. Near-bank PIM architectures place simple cores close to DRAM banks and can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the widely-used, memory-bound Sparse Matrix Vector Multiplication (SpMV) kernel. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make three key contributions. First, we implement a wide variety of software strategies on SpMV for a multithreaded PIM core and characterize the computational limits of a single multithreaded PIM core. Second, we design various load balancing schemes across multiple PIM cores, and two types of data partitioning techniques to execute SpMV on thousands of PIM cores: (1) 1D-partitioned kernels to perform the complete SpMV computation only using PIM cores, and (2) 2D-partitioned kernels to strive a balance between computation and data transfer costs to PIM-enabled memory. Third, we compare SpMV execution on a real-world PIM system with 2528 PIM cores to state-of-the-art CPU and GPU systems to study the performance and energy efficiency of various devices. SparseP software package provides 25 SpMV kernels for real PIM systems supporting the four most widely used compressed matrix formats, and a wide range of data types. Our extensive evaluation provides new insights and recommendations for software designers and hardware architects to efficiently accelerate SpMV on real PIM systems.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Giannoula, Christina and Fernandez, Ivan and Gómez-Luna, Juan and Koziris, Nectarios and Goumas, Georgios and Mutlu, Onur},
	month = may,
	year = {2022},
	note = {arXiv:2201.05072 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Performance},
	annote = {Comment: To appear in the Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS) 2022 and the ACM SIGMETRICS 2022 conference},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\YJMBVDWT\\Giannoula 等 - 2022 - SparseP Towards Efficient Sparse Matrix Vector Mu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\B4E2XSBZ\\2201.html:text/html},
}

@article{lim_design_2023,
	title = {Design and {Analysis} of a {Processing}-in-{DIMM} {Join} {Algorithm}: {A} {Case} {Study} with {UPMEM} {DIMMs}},
	volume = {1},
	shorttitle = {Design and {Analysis} of a {Processing}-in-{DIMM} {Join} {Algorithm}},
	url = {https://doi.org/10.1145/3589258},
	doi = {10.1145/3589258},
	abstract = {Modern dual in-line memory modules (DIMMs) support processing-in-memory (PIM) by implementing in-DIMM processors (IDPs) located near memory banks. PIM can greatly accelerate in-memory join, whose performance is frequently bounded by main-memory accesses, by offloading the operations of join from host central processing units (CPUs) to the IDPs. As real PIM hardware has not been available until very recently, the prior PIM-assisted join algorithms have relied on PIM hardware simulators which assume fast shared memory between the IDPs and fast inter-IDP communication; however, on commodity PIM-enabled DIMMs, the IDPs do not share memory and demand the CPUs to mediate inter-IDP communication. Such discrepancies in the architectural characteristics make the prior studies incompatible with the DIMMs. Thus, to exploit the high potential of PIM on commodity PIM-enabled DIMMs, we need a new join algorithm designed and optimized for the DIMMs and their architectural characteristics. In this paper, we design and analyze Processing-In-DIMM Join (PID-Join), a fast in-memory join algorithm which exploits UPMEM DIMMs, currently the only publicly-available PIM-enabled DIMMs. The DIMMs impose several key challenges on efficient acceleration of join including the shared-nothing nature and limited compute capabilities of the IDPs, the lack of hardware support for fast inter-IDP communication, and the slow IDP-wise data transfers between the IDPs and the main memory. PID-Join overcomes the challenges by prototyping and evaluating hash, sort-merge, and nested-loop algorithms optimized for the IDPs, enabling fast inter-IDP communication using host CPU cache streaming and vector instructions, and facilitating fast rank-wise data transfers between the IDPs and the main memory. Our evaluation using a real system equipped with eight UPMEM DIMMs and 1,024 IDPs shows that PID-Join greatly improves the performance of in-memory join over various CPU-based in-memory join algorithms.},
	number = {2},
	urldate = {2023-10-25},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Lim, Chaemin and Lee, Suhyun and Choi, Jinwoo and Lee, Jounghoo and Park, Seongyeon and Kim, Hanjun and Lee, Jinho and Kim, Youngsok},
	month = jun,
	year = {2023},
	keywords = {processing-in-memory, in-memory join, processing-in-DIMM},
	pages = {113:1--113:27},
}

@misc{gomez-luna_machine_2022,
	title = {Machine {Learning} {Training} on a {Real} {Processing}-in-{Memory} {System}},
	url = {http://arxiv.org/abs/2206.06022},
	doi = {10.48550/arXiv.2206.06022},
	abstract = {Training machine learning algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., computing systems with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck. Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate machine learning training. To do so, we (1) implement several representative classic machine learning algorithms (namely, linear regression, logistic regression, decision tree, K-means clustering) on a real-world general-purpose PIM architecture, (2) characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our experimental evaluation on a memory-centric computing system with more than 2500 PIM cores shows that general-purpose PIM architectures can greatly accelerate memory-bound machine learning workloads, when the necessary operations and datatypes are natively supported by PIM hardware. To our knowledge, our work is the first one to evaluate training of machine learning algorithms on a real-world general-purpose PIM architecture.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Gómez-Luna, Juan and Guo, Yuxin and Brocard, Sylvan and Legriel, Julien and Cimadomo, Remy and Oliveira, Geraldo F. and Singh, Gagandeep and Mutlu, Onur},
	month = aug,
	year = {2022},
	note = {arXiv:2206.06022 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Hardware Architecture},
	annote = {Comment: This extended abstract appears as an invited paper at the 2022 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\9EQRHGZV\\Gómez-Luna 等 - 2022 - Machine Learning Training on a Real Processing-in-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\IGBMFTBS\\2206.html:text/html},
}

@misc{chen_simplepim_2023,
	title = {{SimplePIM}: {A} {Software} {Framework} for {Productive} and {Efficient} {Processing}-in-{Memory}},
	shorttitle = {{SimplePIM}},
	url = {http://arxiv.org/abs/2310.01893},
	doi = {10.48550/arXiv.2310.01893},
	abstract = {Data movement between memory and processors is a major bottleneck in modern computing systems. The processing-in-memory (PIM) paradigm aims to alleviate this bottleneck by performing computation inside memory chips. Real PIM hardware (e.g., the UPMEM system) is now available and has demonstrated potential in many applications. However, programming such real PIM hardware remains a challenge for many programmers. This paper presents a new software framework, SimplePIM, to aid programming real PIM systems. The framework processes arrays of arbitrary elements on a PIM device by calling iterator functions from the host and provides primitives for communication among PIM cores and between PIM and the host system. We implement SimplePIM for the UPMEM PIM system and evaluate it on six major applications. Our results show that SimplePIM enables 66.5\% to 83.1\% reduction in lines of code in PIM programs. The resulting code leads to higher performance (between 10\% and 37\% speedup) than hand-optimized code in three applications and provides comparable performance in three others. SimplePIM is fully and freely available at https://github.com/CMU-SAFARI/SimplePIM.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Chen, Jinfan and Gómez-Luna, Juan and Hajj, Izzat El and Guo, Yuxin and Mutlu, Onur},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01893 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\RLDMHPEX\\Chen 等 - 2023 - SimplePIM A Software Framework for Productive and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\MBHVLGNJ\\2310.html:text/html},
}

@article{mack_performant_2022,
	title = {Performant, {Multi}-{Objective} {Scheduling} of {Highly} {Interleaved} {Task} {Graphs} on {Heterogeneous} {System} on {Chip} {Devices}},
	volume = {33},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/9653796},
	doi = {10.1109/TPDS.2021.3135876},
	abstract = {Performance-, power-, and energy-aware scheduling techniques play an essential role in optimally utilizing processing elements (PEs) of heterogeneous systems. List schedulers, a class of low-complexity static schedulers, have commonly been used in static execution scenarios. However, list schedulers are not suitable for runtime decision making, particularly when multiple concurrent applications are interleaved dynamically. For such cases, the static task execution times and expectation of idle PEs assumed by list schedulers lead to inefficient system utilization and poor performance. To address this problem, we present techniques for optimizing execution of list scheduling algorithms in dynamic runtime scenarios via a family of algorithms inspired by the well-known heterogeneous earliest finish time (HEFT) list scheduler. Through dynamically arriving, realistic workload scenarios that are simulated in an open-source discrete event heterogeneous SoC simulator, we exhaustively evaluate each of the proposed algorithms across two SoCs modeled after the Xilinx Zynq Ultrascale+ ZCU102 and O-Droid XU3 development boards. Altogether, depending on the chosen variant in this family of algorithms, we are able to achieve an up to 39\% execution time improvement, up to 7.24x algorithmic speedup, or up to 30\% energy consumption improvement compared to the baseline HEFT implementation.},
	number = {9},
	urldate = {2023-10-23},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Mack, Joshua and Arda, Samet E. and Ogras, Umit Y. and Akoglu, Ali},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {2148--2162},
	file = {已提交版本:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\478DV363\\Mack 等 - 2022 - Performant, Multi-Objective Scheduling of Highly I.pdf:application/pdf},
}

@article{jafarnejad_ghomi_load-balancing_2017,
	title = {Load-balancing algorithms in cloud computing: {A} survey},
	volume = {88},
	issn = {10848045},
	shorttitle = {Load-balancing algorithms in cloud computing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804517301480},
	doi = {10.1016/j.jnca.2017.04.007},
	abstract = {Cloud computing is a modern paradigm to provide services through the Internet. Load balancing is a key aspect of cloud computing and avoids the situation in which some nodes become overloaded while the others are idle or have little work to do. Load balancing can improve the Quality of Service (QoS) metrics, including response time, cost, throughput, performance and resource utilization.},
	language = {en},
	urldate = {2023-10-17},
	journal = {Journal of Network and Computer Applications},
	author = {Jafarnejad Ghomi, Einollah and Masoud Rahmani, Amir and Nasih Qader, Nooruldeen},
	month = jun,
	year = {2017},
	pages = {50--71},
	file = {Jafarnejad Ghomi 等 - 2017 - Load-balancing algorithms in cloud computing A su.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\INLBCP92\\Jafarnejad Ghomi 等 - 2017 - Load-balancing algorithms in cloud computing A su.pdf:application/pdf},
}

@inproceedings{pandiyan_quantifying_2014,
	title = {Quantifying the energy cost of data movement for emerging smart phone workloads on mobile platforms},
	url = {https://ieeexplore.ieee.org/document/6983056},
	doi = {10.1109/IISWC.2014.6983056},
	abstract = {In portable computing systems like smartphones, energy is generally a key but limited resource where application cores have been proven to consume a significant part of it. To understand the characteristics of the energy consumption, in this paper, we focus our attention on the portion of energy that is spent to move data to the application core's internal registers from the memory system. The primary motivation for this focus comes from the relatively higher energy cost associated with a data movement instruction compared to that of an arithmetic instruction. Another important factor is the distributive computing nature among different units in a SoC which leads to a higher data movement to/from the application cores. We perform a detailed investigation to quantify the impact of data movement on overall energy consumption of a popular, commercially-available smart phone device. To aid this study, we design micro-benchmarks that generate desired data movement patterns between different levels of the memory hierarchy and measure the instantaneous power consumed by the device when running these micro-benchmarks. We extensively make use of hardware performance counters to validate the micro-benchmarks and to characterize the energy consumed in moving data. We take a step further to utilize this calculated energy cost of data movement to characterize the portion of energy that an application spends in moving data for a wide range of popular smart phone workloads. We find that a considerable amount of total device energy is spent in data movement (an average of 35\% of the total device energy). Our results also indicate a relatively high stalled cycle energy consumption (an average of 23.5\%) for current smart phones. To our knowledge, this is the first study that quantifies the amount of data movement energy for emerging smart phone applications running on a recent, commercial smart phone device. We hope this characterization study and the insights developed in the paper can inspire innovative designs in smart phone architectures with improved performance and energy efficiency.},
	urldate = {2023-10-15},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Pandiyan, Dhinakaran and Wu, Carole-Jean},
	month = oct,
	year = {2014},
	pages = {171--180},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\IDP3A9ZP\\6983056.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\GMHUMQFT\\Pandiyan 和 Wu - 2014 - Quantifying the energy cost of data movement for e.pdf:application/pdf},
}

@inproceedings{vogelsang_understanding_2010,
	address = {Atlanta, GA, USA},
	title = {Understanding the {Energy} {Consumption} of {Dynamic} {Random} {Access} {Memories}},
	isbn = {978-1-4244-9071-4},
	url = {http://ieeexplore.ieee.org/document/5695550/},
	doi = {10.1109/MICRO.2010.42},
	abstract = {Energy consumption has become a major constraint on the capabilities of computer systems. In large systems the energy consumed by Dynamic Random Access Memories (DRAM) is a significant part of the total energy consumption. It is possible to calculate the energy consumption of currently available DRAMs from their datasheets, but datasheets don’t allow extrapolation to future DRAM technologies and don’t show how other changes like increasing bandwidth requirements change DRAM energy consumption. This paper first presents a flexible DRAM power model which uses a description of DRAM architecture, technology and operation to calculate power usage and verifies it against datasheet values. Then the model is used together with assumptions about the DRAM roadmap to extrapolate DRAM energy consumption to future DRAM generations. Using this model we evaluate some of the proposed DRAM power reduction schemes.},
	language = {en},
	urldate = {2023-10-15},
	booktitle = {2010 43rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE},
	author = {Vogelsang, Thomas},
	month = dec,
	year = {2010},
	pages = {363--374},
	file = {Vogelsang - 2010 - Understanding the Energy Consumption of Dynamic Ra.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\W7MY2UBD\\Vogelsang - 2010 - Understanding the Energy Consumption of Dynamic Ra.pdf:application/pdf},
}

@article{dennard_design_nodate,
	title = {Design of ion-implanted {MOSFET}'s with very small physical dimensions},
	language = {en},
	author = {Dennard, R H and Gaensslen, F H and Yu, Hwa-Nien and Rideout, V L and Bassous, E and LeBlanc, A R},
	file = {Dennard 等 - Design of ion-implanted MOSFET's with very small p.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\Y867JIFM\\Dennard 等 - Design of ion-implanted MOSFET's with very small p.pdf:application/pdf},
}

@article{moore_cramming_1998,
	title = {Cramming {More} {Components} onto {Integrated} {Circuits}},
	volume = {86},
	language = {en},
	number = {1},
	journal = {PROCEEDINGS OF THE IEEE},
	author = {Moore, Gordon E},
	year = {1998},
	file = {Moore - 1998 - Cramming More Components onto Integrated Circuits.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\HF7Z773A\\Moore - 1998 - Cramming More Components onto Integrated Circuits.pdf:application/pdf},
}

@article{zhang_edge_2023,
	title = {Edge learning using a fully integrated neuro-inspired memristor chip},
	volume = {381},
	url = {https://www.science.org/doi/10.1126/science.ade3483},
	doi = {10.1126/science.ade3483},
	abstract = {Learning is highly important for edge intelligence devices to adapt to different application scenes and owners. Current technologies for training neural networks require moving massive amounts of data between computing and memory units, which hinders the implementation of learning on edge devices. We developed a fully integrated memristor chip with the improvement learning ability and low energy cost. The schemes in the STELLAR architecture, including its learning algorithm, hardware realization, and parallel conductance tuning scheme, are general approaches that facilitate on-chip learning by using a memristor crossbar array, regardless of the type of memristor device. Tasks executed in this study included motion control, image classification, and speech recognition.},
	number = {6663},
	urldate = {2023-10-14},
	journal = {Science},
	author = {Zhang, Wenbin and Yao, Peng and Gao, Bin and Liu, Qi and Wu, Dong and Zhang, Qingtian and Li, Yuankun and Qin, Qi and Li, Jiaming and Zhu, Zhenhua and Cai, Yi and Wu, Dabin and Tang, Jianshi and Qian, He and Wang, Yu and Wu, Huaqiang},
	month = sep,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1205--1211},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\P2JASBD9\\Zhang 等 - 2023 - Edge learning using a fully integrated neuro-inspi.pdf:application/pdf},
}

@inproceedings{lee_1ynm_2022,
	title = {A 1ynm 1.{25V} {8Gb}, {16Gb}/s/pin {GDDR6}-based {Accelerator}-in-{Memory} supporting {1TFLOPS} {MAC} {Operation} and {Various} {Activation} {Functions} for {Deep}-{Learning} {Applications}},
	volume = {65},
	url = {https://ieeexplore.ieee.org/document/9731711},
	doi = {10.1109/ISSCC42614.2022.9731711},
	abstract = {With advances in deep-neural-network applications the increasingly large data movement through memory channels is becoming inevitable: specifically, RNN and MLP applications are memory bound and the memory is the performance bottleneck [1]. DRAM featuring processing in memory (PIM) significantly reduces data movement [1]–[4], and the system performance is enhanced by the large internal parallel bank bandwidth. Among DRAM-based PIM proposals, [3] is near commercialization, but the required HBM technology may prevent it from being applied to other applications due to its high cost [5]. In this situation, an accelerator-in-memory (AiM) based on GDDR6 may be applicable: it has a relatively low-cost, is compatible with GDDR6 interface, and is designed to accelerate deep-learning (DL) applications. AiM offers a peak throughput of 1 TFLOPS with processing units (PUs) with a speed of 1 GHz utilizing the characteristics of GDDR6 with a speed of 16Gb/s. It can also support many applications as it has various activation functions. This paper first looks at the AiM architecture and the supported command set for DL operations. Next, the DL operations in the PU and supported activation functions are described. Finally, we present evaluation results of DL behavior of AiM at the package and the system level.},
	urldate = {2023-10-14},
	booktitle = {2022 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} ({ISSCC})},
	author = {Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and Jeon, Junyeol and Kim, Nahsung and Kwon, Yongkee and Vladimir, Kornijcuk and Shin, Woojae and Won, Jongsoon and Lee, Minkyu and Joo, Hyunha and Choi, Haerang and Lee, Jaewook and Ko, Donguc and Jun, Younggun and Cho, Keewon and Kim, Ilwoong and Song, Choungki and Jeong, Chunseok and Kwon, Daehan and Jang, Jieun and Park, Il and Chun, Junhyun and Cho, Joohwan},
	month = feb,
	year = {2022},
	note = {ISSN: 2376-8606},
	pages = {1--3},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\WWYIKWTR\\9731711.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\KJIX5AQT\\Lee 等 - 2022 - A 1ynm 1.25V 8Gb, 16Gbspin GDDR6-based Accelerat.pdf:application/pdf},
}

@inproceedings{lee_improving_2022,
	address = {New York, NY, USA},
	series = {{DaMoN} '22},
	title = {Improving {In}-{Memory} {Database} {Operations} with {Acceleration} {DIMM} ({AxDIMM})},
	isbn = {978-1-4503-9378-2},
	url = {https://dl.acm.org/doi/10.1145/3533737.3535093},
	doi = {10.1145/3533737.3535093},
	abstract = {The significant overhead needed to transfer the data between CPUs and memory devices is one of the hottest issues in many areas of computing, such as database management systems. Disaggregated computing on the memory devices is being highlighted as one promising approach. In this work, we introduce a new near-memory acceleration scheme for in-memory database operations, called Acceleration DIMM (AxDIMM). It behaves like a normal DIMM through the standard DIMM-compatible interface, but has embedded computing units for data-intensive operations. With the minimized data transfer overhead, it reduces CPU resource consumption, relieves the memory bandwidth bottleneck, and boosts energy efficiency. We implement scan operations, one of the most data-intensive database operations, within AxDIMM and compare its performance with SIMD (Single Instruction Multiple Data) implementation on CPU. Our investigation shows that the acceleration achieves 6.8x more throughput than the SIMD implementation.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 18th {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Donghun and So, Jinin and AHN, MINSEON and Lee, Jong-Geon and Kim, Jungmin and Cho, Jeonghyeon and Oliver, Rebholz and Thummala, Vishnu Charan and JV, Ravi shankar and Upadhya, Sachin Suresh and Khan, Mohammed Ibrahim and Kim, Jin Hyun},
	month = jun,
	year = {2022},
	keywords = {Acceleration DIMM, AxDIMM, Database Management Systems, DBMS, In-Memory Database, Near-Memory Processing},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\9RWAZ3SW\\Lee 等 - 2022 - Improving In-Memory Database Operations with Accel.pdf:application/pdf},
}

@article{ke_near-memory_2022,
	title = {Near-{Memory} {Processing} in {Action}: {Accelerating} {Personalized} {Recommendation} {With} {AxDIMM}},
	volume = {42},
	issn = {1937-4143},
	shorttitle = {Near-{Memory} {Processing} in {Action}},
	url = {https://ieeexplore.ieee.org/abstract/document/9489313},
	doi = {10.1109/MM.2021.3097700},
	abstract = {Near-memory processing (NMP) is a prospective paradigm enabling memory-centric computing. By moving the compute capability next to the main memory (DRAM modules), it can fundamentally address the CPU-memory bandwidth bottleneck and thus effectively improve the performance of memory-constrained workloads. Using the personalized recommendation system as a driving example, we developed a scalable, practical DIMM-based NMP solution tailor-designed for accelerating the inference serving. Our solution is demonstrated on a versatile FPGA-enabled NMP platform called AxDIMM that allows rapid prototyping and evaluation of NMP’s performance potential on real hardware under a realistic system setting using industry-representative recommendation framework. We experimentally validated the performance of a two-ranked AxDIMM prototype, which achieves up to 1.89× speedup in latency and 31.6\% memory energy saving for embedding operations. For end-to-end recommendation inference serving, AxDIMM improves the throughput up to 1.5× and latency-bounded throughput up to 1.77×, respectively.},
	number = {1},
	urldate = {2023-10-14},
	journal = {IEEE Micro},
	author = {Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Micro},
	pages = {116--127},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\2JTZYDCU\\9489313.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\5LX8F9HC\\Ke 等 - 2022 - Near-Memory Processing in Action Accelerating Per.pdf:application/pdf},
}

@inproceedings{lee_hardware_2021,
	address = {Valencia, Spain},
	title = {Hardware {Architecture} and {Software} {Stack} for {PIM} {Based} on {Commercial} {DRAM} {Technology} : {Industrial} {Product}},
	isbn = {978-1-66543-333-4},
	shorttitle = {Hardware {Architecture} and {Software} {Stack} for {PIM} {Based} on {Commercial} {DRAM} {Technology}},
	url = {https://ieeexplore.ieee.org/document/9499894/},
	doi = {10.1109/ISCA52012.2021.00013},
	abstract = {Emerging applications such as deep neural network demand high off-chip memory bandwidth. However, under stringent physical constraints of chip packages and system boards, it becomes very expensive to further increase the bandwidth of off-chip memory. Besides, transferring data across the memory hierarchy constitutes a large fraction of total energy consumption of systems, and the fraction has steadily increased with the stagnant technology scaling and poor data reuse characteristics of such emerging applications. To cost-effectively increase the bandwidth and energy efﬁciency, researchers began to reconsider the past processing-in-memory (PIM) architectures and advance them further, especially exploiting recent integration technologies such as 2.5D/3D stacking. Albeit the recent advances, no major memory manufacturer has developed even a proof-of-concept silicon yet, not to mention a product. This is because the past PIM architectures often require changes in host processors and/or application code which memory manufacturers cannot easily govern. In this paper, elegantly tackling the aforementioned challenges, we propose an innovative yet practical PIM architecture. To demonstrate its practicality and effectiveness at the system level, we implement it with a 20nm DRAM technology, integrate it with an unmodiﬁed commercial processor, develop the necessary software stack, and run existing applications without changing their source code. Our evaluation at the system level shows that our PIM improves the performance of memory-bound neural network kernels and applications by 11.2× and 3.5×, respectively. Atop the performance improvement, PIM also reduces the energy per bit transfer by 3.5×, and the overall energy efﬁciency of the system running the applications by 3.2×.},
	language = {en},
	urldate = {2023-10-14},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung},
	month = jun,
	year = {2021},
	pages = {43--56},
	file = {Lee 等 - 2021 - Hardware Architecture and Software Stack for PIM B.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\7LRLGKS7\\Lee 等 - 2021 - Hardware Architecture and Software Stack for PIM B.pdf:application/pdf},
}

@inproceedings{kwon_254_2021,
	address = {San Francisco, CA, USA},
	title = {25.4 {A} 20nm {6GB} {Function}-{In}-{Memory} {DRAM}, {Based} on {HBM2} with a 1.{2TFLOPS} {Programmable} {Computing} {Unit} {Using} {Bank}-{Level} {Parallelism}, for {Machine} {Learning} {Applications}},
	isbn = {978-1-72819-549-0},
	url = {https://ieeexplore.ieee.org/document/9365862/},
	doi = {10.1109/ISSCC42613.2021.9365862},
	language = {en},
	urldate = {2023-10-14},
	booktitle = {2021 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} ({ISSCC})},
	publisher = {IEEE},
	author = {Kwon, Young-Cheon and Lee, Suk Han and Lee, Jaehoon and Kwon, Sang-Hyuk and Ryu, Je Min and Son, Jong-Pil and Seongil, O and Yu, Hak-Soo and Lee, Haesuk and Kim, Soo Young and Cho, Youngmin and Kim, Jin Guk and Choi, Jongyoon and Shin, Hyun-Sung and Kim, Jin and Phuah, BengSeng and Kim, HyoungMin and Song, Myeong Jun and Choi, Ahn and Kim, Daeho and Kim, SooYoung and Kim, Eun-Bong and Wang, David and Kang, Shinhaeng and Ro, Yuhwan and Seo, Seungwoo and Song, JoonHo and Youn, Jaeyoun and Sohn, Kyomin and Kim, Nam Sung},
	month = feb,
	year = {2021},
	pages = {350--352},
	file = {Kwon 等 - 2021 - 25.4 A 20nm 6GB Function-In-Memory DRAM, Based on .pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\DQPLDNMK\\Kwon 等 - 2021 - 25.4 A 20nm 6GB Function-In-Memory DRAM, Based on .pdf:application/pdf},
}

@article{denning_exponential_2016,
	title = {Exponential laws of computing growth},
	volume = {60},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/2976758},
	doi = {10.1145/2976758},
	abstract = {Moore's Law is one small component in an exponentially growing planetary computing ecosystem.},
	number = {1},
	urldate = {2023-10-14},
	journal = {Communications of the ACM},
	author = {Denning, Peter J. and Lewis, Ted G.},
	month = dec,
	year = {2016},
	pages = {54--65},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\XS9B77SW\\Denning 和 Lewis - 2016 - Exponential laws of computing growth.pdf:application/pdf},
}

@inproceedings{jun_hbm_2017,
	title = {{HBM} ({High} {Bandwidth} {Memory}) {DRAM} {Technology} and {Architecture}},
	url = {https://ieeexplore.ieee.org/document/7939084},
	doi = {10.1109/IMW.2017.7939084},
	abstract = {HBM (High Bandwidth Memory) is an emerging standard DRAM solution that can achieve breakthrough bandwidth of higher than 256GBps while reducing the power consumption as well. It has stacked DRAM architecture with core DRAM dies on top of a base logic die, based on the TSV and die stacking technologies. In this paper, the HBM architecture is introduced and a comparison of its generations is provided. Also, the packaging technology and challenges to address reliability, thermal dissipation capability, maximum allowable package sizes, and high throughput stacking solutions are described. Test technology and testability features are discussed for KGSD and 2.5D SiP.},
	urldate = {2023-10-14},
	booktitle = {2017 {IEEE} {International} {Memory} {Workshop} ({IMW})},
	author = {Jun, Hongshin and Cho, Jinhee and Lee, Kangseol and Son, Ho-Young and Kim, Kwiwook and Jin, Hanho and Kim, Keith},
	month = may,
	year = {2017},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\GG9LKPYM\\7939084.html:text/html},
}

@article{boroumand_lazypim_2017,
	title = {{LazyPIM}: {An} {Efficient} {Cache} {Coherence} {Mechanism} for {Processing}-in-{Memory}},
	volume = {16},
	issn = {1556-6056},
	shorttitle = {{LazyPIM}},
	url = {http://ieeexplore.ieee.org/document/7485993/},
	doi = {10.1109/LCA.2016.2577557},
	abstract = {Processing-in-memory (PIM) architectures cannot use traditional approaches to cache coherence due to the high off-chip trafﬁc consumed by coherence messages. We propose LazyPIM, a new hardware cache coherence mechanism designed speciﬁcally for PIM. LazyPIM uses a combination of speculative cache coherence and compressed coherence signatures to greatly reduce the overhead of keeping PIM coherent with the processor. We ﬁnd that LazyPIM improves average performance across a range of PIM applications by 49.1\% over the best prior approach, coming within 5.5\% of an ideal PIM mechanism.},
	language = {en},
	number = {1},
	urldate = {2023-10-14},
	journal = {IEEE Computer Architecture Letters},
	author = {Boroumand, Amirali and Ghose, Saugata and Patel, Minesh and Hassan, Hasan and Lucia, Brandon and Hsieh, Kevin and Malladi, Krishna T. and Zheng, Hongzhong and Mutlu, Onur},
	month = jan,
	year = {2017},
	pages = {46--50},
	file = {Boroumand 等 - 2017 - LazyPIM An Efficient Cache Coherence Mechanism fo.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\IEXMCQYL\\Boroumand 等 - 2017 - LazyPIM An Efficient Cache Coherence Mechanism fo.pdf:application/pdf},
}

@article{hashemi_accelerating_nodate,
	title = {Accelerating {Dependent} {Cache} {Misses} with an {Enhanced} {Memory} {Controller}},
	abstract = {On-chip contention increases memory access latency for multicore processors. We identify that this additional latency has a substantial e ect on performance for an important class of latency-critical memory operations: those that result in a cache miss and are dependent on data from a prior cache miss. We observe that the number of instructions between the rst cache miss and its dependent cache miss is usually small. To minimize dependent cache miss latency, we propose adding just enough functionality to dynamically identify these instructions at the core and migrate them to the memory controller for execution as soon as source data arrives from DRAM. This migration allows memory requests issued by our new Enhanced Memory Controller (EMC) to experience a 20\% lower latency than if issued by the core. On a set of memory intensive quad-core workloads, the EMC results in a 13\% improvement in system performance and a 5\% reduction in energy consumption over a system with a Global History Bu er prefetcher, the highest performing prefetcher in our evaluation.},
	language = {en},
	author = {Hashemi, Milad and Ebrahimi, Eiman and Mutlu, Onur and Patt, Yale N},
	file = {Hashemi 等 - Accelerating Dependent Cache Misses with an Enhanc.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\WYQVIRDZ\\Hashemi 等 - Accelerating Dependent Cache Misses with an Enhanc.pdf:application/pdf},
}

@article{chang_energy-efficient_2021,
	title = {Energy-efficient computing-in-memory architecture for {AI} processor: device, circuit, architecture perspective},
	volume = {64},
	issn = {1869-1919},
	shorttitle = {Energy-efficient computing-in-memory architecture for {AI} processor},
	url = {https://doi.org/10.1007/s11432-021-3234-0},
	doi = {10.1007/s11432-021-3234-0},
	abstract = {An artificial intelligence (AI) processor is a promising solution for energy-efficient data processing, including health monitoring and image/voice recognition. However, data movements between compute part and memory induce memory wall and power wall challenges to the conventional computing architecture. Recently, the memory-centric architecture has been revised to solve the data movement issue, where the memory is equipped with the compute-capable memory technique, namely, computing-in-memory (CIM). In this paper, we analyze the requirement of AI algorithms on the data movement and low power requirement of AI processors. In addition, we introduce the story of CIM and implementation methodologies of CIM architecture. Furthermore, we present several novel solutions beyond traditional analog-digital mixed static random-access memory (SRAM)-based CIM architecture. Finally, recent CIM tape-out studies are listed and discussed.},
	language = {en},
	number = {6},
	urldate = {2023-10-06},
	journal = {Science China Information Sciences},
	author = {Chang, Liang and Li, Chenglong and Zhang, Zhaomin and Xiao, Jianbiao and Liu, Qingsong and Zhu, Zhen and Li, Weihang and Zhu, Zixuan and Yang, Siqi and Zhou, Jun},
	month = may,
	year = {2021},
	keywords = {AI processor, computing-in-memory, energy efficiency, non-volatile memory, test demonstrators},
	pages = {160403},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\HAFMUDA8\\Chang 等 - 2021 - Energy-efficient computing-in-memory architecture .pdf:application/pdf},
}

@article{lee_skmd_2015,
	title = {{SKMD}: {Single} {Kernel} on {Multiple} {Devices} for {Transparent} {CPU}-{GPU} {Collaboration}},
	volume = {33},
	issn = {0734-2071, 1557-7333},
	shorttitle = {{SKMD}},
	url = {https://dl.acm.org/doi/10.1145/2798725},
	doi = {10.1145/2798725},
	abstract = {Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. This work distribution can be a poor solution as it underutilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this article, we present the single-kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single data-parallel kernel in OpenCL, while the system automatically partitions the workload across an arbitrary set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 28\% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.},
	language = {en},
	number = {3},
	urldate = {2023-12-15},
	journal = {ACM Transactions on Computer Systems},
	author = {Lee, Janghaeng and Samadi, Mehrzad and Park, Yongjun and Mahlke, Scott},
	month = sep,
	year = {2015},
	pages = {1--27},
	file = {Lee 等 - 2015 - SKMD Single Kernel on Multiple Devices for Transp.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\VYJFIUAR\\Lee 等 - 2015 - SKMD Single Kernel on Multiple Devices for Transp.pdf:application/pdf},
}

@article{press_hedges_2020,
	title = {{HEDGES} error-correcting code for {DNA} storage corrects indels and allows sequence constraints},
	volume = {117},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2004821117},
	doi = {10.1073/pnas.2004821117},
	abstract = {Synthetic DNA is rapidly emerging as a durable, high-density information storage platform. A major challenge for DNA-based information encoding strategies is the high rate of errors that arise during DNA synthesis and sequencing. Here, we describe the HEDGES (Hash Encoded, Decoded by Greedy Exhaustive Search) error-correcting code that repairs all three basic types of DNA errors: insertions, deletions, and substitutions. HEDGES also converts unresolved or compound errors into substitutions, restoring synchronization for correction via a standard Reed–Solomon outer code that is interleaved across strands. Moreover, HEDGES can incorporate a broad class of user-defined sequence constraints, such as avoiding excess repeats, or too high or too low windowed guanine–cytosine (GC) content. We test our code both via in silico simulations and with synthesized DNA. From its measured performance, we develop a statistical model applicable to much larger datasets. Predicted performance indicates the possibility of error-free recovery of petabyte- and exabyte-scale data from DNA degraded with as much as 10\% errors. As the cost of DNA synthesis and sequencing continues to drop, we anticipate that HEDGES will find applications in large-scale error-free information encoding.},
	number = {31},
	urldate = {2024-01-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Press, William H. and Hawkins, John A. and Jones, Stephen K. and Schaub, Jeffrey M. and Finkelstein, Ilya J.},
	month = aug,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {18489--18496},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\BEVW9CTC\\Press 等 - 2020 - HEDGES error-correcting code for DNA storage corre.pdf:application/pdf},
}

@article{baker_tag_invoke_nodate,
	title = {tag\_invoke: {A} general pattern for supporting customisable functions},
	language = {en},
	author = {Baker, Lewis and Niebler, Eric and Shoop, Kirk},
	file = {Baker 等 - tag_invoke A general pattern for supporting custo.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\L7SLK7ZD\\Baker 等 - tag_invoke A general pattern for supporting custo.pdf:application/pdf},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\58UY7AXJ\\Paszke 等 - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\54CUBCHD\\1912.html:text/html},
}

@article{abadi_tensorflow_nodate,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	language = {en},
	author = {Abadi, Martın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	file = {Abadi 等 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\SVGBJZ53\\Abadi 等 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf},
}

@inproceedings{chen_xgboost_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1603.02754 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {785--794},
	annote = {Comment: KDD'16 changed all figures to type1},
	file = {arXiv Fulltext PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\PH75ER4P\\Chen 和 Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\PJPYRZN3\\1603.html:text/html},
}

@inproceedings{fernandez_natsa_2020,
	title = {{NATSA}: {A} {Near}-{Data} {Processing} {Accelerator} for {Time} {Series} {Analysis}},
	shorttitle = {{NATSA}},
	url = {https://ieeexplore.ieee.org/document/9283516},
	doi = {10.1109/ICCD50377.2020.00035},
	abstract = {Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2× (9.9× on average) and reduces energy by up to 27.2 × (19.4 × on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 × and reduces energy by 10.2 × over a general-purpose NDP platform with 64 in-order cores.},
	urldate = {2024-03-03},
	booktitle = {2020 {IEEE} 38th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Fernandez, Ivan and Quislant, Ricardo and Gutiérrez, Eladio and Plata, Oscar and Giannoula, Christina and Alser, Mohammed and Gómez-Luna, Juan and Mutlu, Onur},
	month = oct,
	year = {2020},
	note = {ISSN: 2576-6996},
	keywords = {Bandwidth, Computational efficiency, Energy efficiency, Memory management, Prediction algorithms, System-on-chip, Time series analysis},
	pages = {120--129},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\V6M2GIUW\\9283516.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\2V9ENYHW\\Fernandez 等 - 2020 - NATSA A Near-Data Processing Accelerator for Time.pdf:application/pdf},
}


@inproceedings{fernandez_natsa_2020,
	title = {{NATSA}: {A} {Near}-{Data} {Processing} {Accelerator} for {Time} {Series} {Analysis}},
	shorttitle = {{NATSA}},
	url = {https://ieeexplore.ieee.org/document/9283516},
	doi = {10.1109/ICCD50377.2020.00035},
	abstract = {Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2× (9.9× on average) and reduces energy by up to 27.2 × (19.4 × on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 × and reduces energy by 10.2 × over a general-purpose NDP platform with 64 in-order cores.},
	urldate = {2024-03-03},
	booktitle = {2020 {IEEE} 38th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Fernandez, Ivan and Quislant, Ricardo and Gutiérrez, Eladio and Plata, Oscar and Giannoula, Christina and Alser, Mohammed and Gómez-Luna, Juan and Mutlu, Onur},
	month = oct,
	year = {2020},
	note = {ISSN: 2576-6996},
	keywords = {Bandwidth, Computational efficiency, Energy efficiency, Memory management, Prediction algorithms, System-on-chip, Time series analysis},
	pages = {120--129},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\V6M2GIUW\\9283516.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\2V9ENYHW\\Fernandez 等 - 2020 - NATSA A Near-Data Processing Accelerator for Time.pdf:application/pdf},
}

@article{wulf_hitting_1995,
	title = {Hitting the memory wall: implications of the obvious},
	volume = {23},
	issn = {0163-5964},
	shorttitle = {Hitting the memory wall},
	url = {https://dl.acm.org/doi/10.1145/216585.216588},
	doi = {10.1145/216585.216588},
	language = {en},
	number = {1},
	urldate = {2024-03-06},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Wulf, Wm. A. and McKee, Sally A.},
	month = mar,
	year = {1995},
	pages = {20--24},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\ENEFH452\\Wulf 和 McKee - 1995 - Hitting the memory wall implications of the obvio.pdf:application/pdf},
}

@inproceedings{clapp_quantifying_2015,
	address = {Atlanta, GA, USA},
	title = {Quantifying the {Performance} {Impact} of {Memory} {Latency} and {Bandwidth} for {Big} {Data} {Workloads}},
	isbn = {978-1-5090-0088-3},
	url = {http://ieeexplore.ieee.org/document/7314167/},
	doi = {10.1109/IISWC.2015.32},
	urldate = {2024-03-06},
	booktitle = {2015 {IEEE} {International} {Symposium} on {Workload} {Characterization}},
	publisher = {IEEE},
	author = {Clapp, Russell and Dimitrov, Martin and Kumar, Karthik and Viswanathan, Vish and Willhalm, Thomas},
	month = oct,
	year = {2015},
	pages = {213--224},
	file = {Clapp 等 - 2015 - Quantifying the Performance Impact of Memory Laten.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\JUNSS8TK\\Clapp 等 - 2015 - Quantifying the Performance Impact of Memory Laten.pdf:application/pdf},
}

@misc{gholami_ai_2023,
	title = {{AI} and {Memory} {Wall}},
	url = {https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8},
	abstract = {(This blogpost has been written in collaboration with Zhewei Yao, Sehoon Kim, Michael W. Mahoney, and Kurt Keutzer. The data used for this…},
	language = {en},
	urldate = {2024-03-06},
	journal = {riselab},
	author = {Gholami, Amir},
	month = dec,
	year = {2023},
	file = {Snapshot:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\4UN67NE6\\ai-and-memory-wall-2cb4265cb0b8.html:text/html},
}

@inproceedings{sun_abc-dimm_2021,
	address = {Valencia, Spain},
	title = {{ABC}-{DIMM}: {Alleviating} the {Bottleneck} of {Communication} in {DIMM}-based {Near}-{Memory} {Processing} with {Inter}-{DIMM} {Broadcast}},
	isbn = {978-1-66543-333-4},
	shorttitle = {{ABC}-{DIMM}},
	url = {https://ieeexplore.ieee.org/document/9499805/},
	doi = {10.1109/ISCA52012.2021.00027},
	abstract = {Near-Memory Processing (NMP) systems that integrate accelerators within DIMM (Dual-Inline Memory Module) buffer chips potentially provide high performance with relatively low design and manufacturing costs. However, an inevitable communication bottleneck arises when considering the main memory bus among peer DIMMs and the host CPU. This communication bottleneck roots in the bus-based nature and the limited point-to-point communication pattern of the main memory system. The aggregated memory bandwidth of DIMMbased NMP scales with the number of DIMMs. When the number of DIMMs in a channel scales up, the per-DIMM point-to-point communication bandwidth scales down, whereas the computation resources and local memory bandwidth per DIMM stay the same. For many important sparse data-intensive workloads like graph applications and sparse tensor algebra, we identify that communication among DIMMs and the host CPU easily dominates their processing procedure in previous DIMM-based NMP systems, which severely bottlenecks their performance.},
	language = {en},
	urldate = {2024-03-06},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Sun, Weiyi and Li, Zhaoshi and Yin, Shouyi and Wei, Shaojun and Liu, Leibo},
	month = jun,
	year = {2021},
	pages = {237--250},
	file = {Sun 等 - 2021 - ABC-DIMM Alleviating the Bottleneck of Communicat.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\FSR99UHM\\Sun 等 - 2021 - ABC-DIMM Alleviating the Bottleneck of Communicat.pdf:application/pdf},
}

@inproceedings{bernhardt_pimdb_2023,
	address = {New York, NY, USA},
	series = {{DaMoN} '23},
	title = {{pimDB}: {From} {Main}-{Memory} {DBMS} to {Processing}-{In}-{Memory} {DBMS}-{Engines} on {Intelligent} {Memories}},
	isbn = {9798400701917},
	shorttitle = {{pimDB}},
	url = {https://dl.acm.org/doi/10.1145/3592980.3595312},
	doi = {10.1145/3592980.3595312},
	abstract = {The performance and scalability of modern data-intensive systems are limited by massive data movement of growing datasets across the whole memory hierarchy to the CPUs. Such traditional processor-centric DBMS architectures are bandwidth- and latency-bound. Processing-in-Memory (PIM) designs seek to overcome these limitations by integrating memory and processing functionality on the same chip. PIM targets near- or in-memory data processing, leveraging the greater in-situ parallelism and bandwidth. In this paper, we introduce pimDB and provide an initial comparison of processor-centric and PIM-DBMS approaches under different aspects, such as scalability and parallelism, cache-awareness, or PIM-specific compute/bandwidth tradeoffs. The evaluation is performed end-to-end on a real PIM hardware system from UPMEM.},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the 19th {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {Association for Computing Machinery},
	author = {Bernhardt, Arthur and Koch, Andreas and Petrov, Ilia},
	month = jun,
	year = {2023},
	pages = {44--52},
	file = {Full Text PDF:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\WKG5QRDM\\Bernhardt 等 - 2023 - pimDB From Main-Memory DBMS to Processing-In-Memo.pdf:application/pdf},
}

@article{deelman_pegasus_2005,
	title = {Pegasus: {A} {Framework} for {Mapping} {Complex} {Scientific} {Workflows} onto {Distributed} {Systems}},
	volume = {13},
	issn = {1058-9244, 1875-919X},
	shorttitle = {Pegasus},
	url = {http://www.hindawi.com/journals/sp/2005/128026/abs/},
	doi = {10.1155/2005/128026},
	abstract = {This paper describes the Pegasus framework that can be used to map complex scientiﬁc workﬂows onto distributed resources. Pegasus enables users to represent the workﬂows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workﬂow restructuring which clusters multiple tasks in a workﬂow into single entities. A real-life astronomy application is used as the basis for the study.},
	language = {en},
	number = {3},
	urldate = {2024-03-07},
	journal = {Scientific Programming},
	author = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G. Bruce and Good, John and Laity, Anastasia and Jacob, Joseph C. and Katz, Daniel S.},
	year = {2005},
	pages = {219--237},
	file = {Deelman 等 - 2005 - Pegasus A Framework for Mapping Complex Scientifi.pdf:C\:\\Users\\a6174\\OneDrive\\zotero\\storage\\VHLCMNY5\\Deelman 等 - 2005 - Pegasus A Framework for Mapping Complex Scientifi.pdf:application/pdf},
}
